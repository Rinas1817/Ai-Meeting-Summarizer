{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGVC8rof/VRC38Yx71N7+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rinas1817/Ai-Meeting-Summarizer/blob/main/AI_Summarizer_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL5ZMP5xoGkh"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# MASTER SETUP CELL (V9 - Final Fix): Run this\n",
        "# ================================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. MOUNT GOOGLE DRIVE\n",
        "print(\"‚ñ∂Ô∏è Mounting Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 2. DEFINE PROJECT PATHS\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/AI_Summarizer_Project\"\n",
        "MODEL_PATH = os.path.join(DRIVE_ROOT, \"models\")\n",
        "PIP_CACHE_DIR = os.path.join(DRIVE_ROOT, \"pip_cache\")\n",
        "HF_CACHE_DIR = os.path.join(DRIVE_ROOT, \"hf_cache\")\n",
        "VOICEPRINT_PATH = os.path.join(DRIVE_ROOT, \"voiceprints\")\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "os.makedirs(PIP_CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(HF_CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(VOICEPRINT_PATH, exist_ok=True)\n",
        "print(\"‚úÖ Drive mounted and paths defined.\")\n",
        "\n",
        "# 3. INSTALL DEPENDENCIES\n",
        "print(\"\\n‚ñ∂Ô∏è Installing dependencies...\")\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install --cache-dir={PIP_CACHE_DIR} -q --upgrade pip\n",
        "!pip install --cache-dir={PIP_CACHE_DIR} -q git+https://github.com/openai/whisper.git\n",
        "!pip install --cache-dir={PIP_CACHE_DIR} -q streamlit pyngrok pyannote.audio==3.1.1 google-generativeai python-dotenv scipy pydub\n",
        "!pip install numpy==1.26.4\n",
        "print(\"‚úÖ Dependencies installed.\")\n",
        "\n",
        "# 4. LOAD MODELS\n",
        "print(\"\\n‚ñ∂Ô∏è Loading AI models...\")\n",
        "os.environ['HUGGING_FACE_HUB_CACHE'] = HF_CACHE_DIR\n",
        "from google.colab import userdata\n",
        "\n",
        "import whisper\n",
        "import torch\n",
        "whisper_model = whisper.load_model(\"base\", download_root=MODEL_PATH)\n",
        "print(\"‚úÖ Whisper model loaded.\")\n",
        "\n",
        "from pyannote.audio import Pipeline, Model # <--- Import Model\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN)\n",
        "\n",
        "# THE FINAL FIX: Load this as a Model, not a Pipeline\n",
        "embedding_model = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=HF_TOKEN)\n",
        "\n",
        "print(\"‚úÖ Pyannote models loaded.\")\n",
        "\n",
        "# 5. CONFIGURE GEMINI API\n",
        "print(\"\\n‚ñ∂Ô∏è Configuring Gemini API...\")\n",
        "import google.generativeai as genai\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "print(\"‚úÖ Gemini client initialized.\")\n",
        "\n",
        "print(\"\\n\\nüéâ SETUP COMPLETE! You are ready to go. üéâ\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# ================================================================\n",
        "# THE COMPLETE AND FINAL STREAMLIT APPLICATION\n",
        "# ================================================================\n",
        "import streamlit as st\n",
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "import uuid\n",
        "from pydub import AudioSegment\n",
        "import io\n",
        "import json\n",
        "import whisper\n",
        "from pyannote.audio import Pipeline, Model, Inference\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- CONFIGURATION AND PATHS ---\n",
        "st.set_page_config(page_title=\"AI Meeting Summarizer\", layout=\"wide\")\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/AI_Summarizer_Project\"\n",
        "VOICEPRINT_PATH = os.path.join(DRIVE_ROOT, \"voiceprints\")\n",
        "MODEL_PATH = os.path.join(DRIVE_ROOT, \"models\")\n",
        "HF_CACHE_DIR = os.path.join(DRIVE_ROOT, \"hf_cache\")\n",
        "os.environ['HUGGING_FACE_HUB_CACHE'] = HF_CACHE_DIR\n",
        "\n",
        "# --- LOAD MODELS AND API CLIENTS (CACHED) ---\n",
        "@st.cache_resource\n",
        "def load_models_and_clients():\n",
        "    from google.colab import userdata\n",
        "    # Reads keys passed from the launcher cell\n",
        "    HF_TOKEN = os.getenv('HF_TOKEN') or userdata.get('HF_TOKEN')\n",
        "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY') or userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "    if not HF_TOKEN or not GEMINI_API_KEY:\n",
        "        raise ValueError(\"API keys for Hugging Face and/or Gemini are not set in the environment.\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    st.info(f\"Using device: {device}\")\n",
        "\n",
        "    whisper_model = whisper.load_model(\"base\", device=device, download_root=MODEL_PATH)\n",
        "    diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN).to(device)\n",
        "    embedding_model = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=HF_TOKEN).to(device)\n",
        "    embedding_inference = Inference(embedding_model, window=\"whole\", device=device)\n",
        "\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    generation_config = {\"temperature\": 0.7, \"top_p\": 1, \"top_k\": 1, \"max_output_tokens\": 8192}\n",
        "    gemini_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
        "\n",
        "    return whisper_model, diarization_pipeline, embedding_inference, gemini_model\n",
        "\n",
        "try:\n",
        "    whisper_model, diarization_pipeline, embedding_inference, gemini_model = load_models_and_clients()\n",
        "    models_loaded = True\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to load AI models. Please check your API keys and file paths. Error: {e}\")\n",
        "    models_loaded = False\n",
        "\n",
        "# --- GEMINI PROMPT ---\n",
        "GEMINI_JSON_PROMPT = \"\"\"\n",
        "You are an expert meeting analysis AI. Your task is to analyze the provided meeting transcript and extract key information. Your response MUST be a single, valid JSON object. Do not include any text or formatting before or after the JSON object. The JSON object must have the following keys:\n",
        "- \"title\": A short, catchy title for the meeting (e.g., \"Q3 Marketing Strategy Session\").\n",
        "- \"summary\": A concise, one-paragraph summary of the meeting's key discussions and outcomes.\n",
        "- \"action_items\": An array of objects. Each object represents a specific task and must have the keys: \"task\", \"assigned_to\", \"deadline\". If info is missing, use \"Not specified\".\n",
        "- \"key_decisions\": An array of strings, where each string is a key decision made during the meeting.\n",
        "- \"topics_discussed\": An array of strings, where each string is a main topic that was discussed.\n",
        "Analyze the following transcript carefully:\n",
        "TRANSCRIPT:\n",
        "'''\n",
        "[INSERT TRANSCRIPT HERE]\n",
        "'''\n",
        "\"\"\"\n",
        "\n",
        "# --- CORE LOGIC FUNCTIONS ---\n",
        "def get_embedding(file_path):\n",
        "    return embedding_inference(file_path)\n",
        "\n",
        "def enroll_speaker(speaker_name, audio_file):\n",
        "    temp_file_path = f\"temp_enroll_{uuid.uuid4()}.wav\"\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(io.BytesIO(audio_file.read()))\n",
        "        audio.export(temp_file_path, format=\"wav\")\n",
        "        voiceprint = get_embedding(temp_file_path)\n",
        "        speaker_data = {\"name\": speaker_name, \"voiceprint\": voiceprint}\n",
        "        file_path = os.path.join(VOICEPRINT_PATH, f\"{speaker_name}_{uuid.uuid4()}.pkl\")\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            pickle.dump(speaker_data, f)\n",
        "        st.session_state.enrolled_speakers = load_enrolled_speakers()\n",
        "        return f\"‚úÖ Successfully enrolled **{speaker_name}**.\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error during enrollment: {e}\"\n",
        "    finally:\n",
        "        if os.path.exists(temp_file_path):\n",
        "            os.remove(temp_file_path)\n",
        "\n",
        "def load_enrolled_speakers():\n",
        "    enrolled_speakers = []\n",
        "    if not os.path.exists(VOICEPRINT_PATH): return []\n",
        "    for filename in os.listdir(VOICEPRINT_PATH):\n",
        "        if filename.endswith(\".pkl\"):\n",
        "            file_path = os.path.join(VOICEPRINT_PATH, filename)\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                enrolled_speakers.append(pickle.load(f))\n",
        "    return enrolled_speakers\n",
        "\n",
        "def process_meeting(meeting_audio_file):\n",
        "    diarize_temp_path = f\"temp_diarize_{uuid.uuid4()}.wav\"\n",
        "\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(io.BytesIO(meeting_audio_file.read()))\n",
        "        audio.export(diarize_temp_path, format=\"wav\")\n",
        "\n",
        "        enrolled_speakers = st.session_state.get('enrolled_speakers', [])\n",
        "        diarization = diarization_pipeline(diarize_temp_path)\n",
        "\n",
        "        # --- NEW LOGIC WITH SIMILARITY THRESHOLD ---\n",
        "        speaker_mapping = {}\n",
        "        DISTANCE_THRESHOLD = 0.5\n",
        "\n",
        "        if enrolled_speakers:\n",
        "            st.info(\"Enrolled speakers found. Attempting to identify them...\")\n",
        "            unidentified_speakers = {}\n",
        "            generic_speaker_count = 1\n",
        "\n",
        "            for turn, _, speaker_label in diarization.itertracks(yield_label=True):\n",
        "                if speaker_label not in speaker_mapping:\n",
        "                    temp_segment_path = f\"temp_segment_{uuid.uuid4()}.wav\"\n",
        "                    segment = audio[turn.start * 1000 : turn.end * 1000]\n",
        "                    segment.export(temp_segment_path, format=\"wav\")\n",
        "\n",
        "                    current_embedding = get_embedding(temp_segment_path)\n",
        "\n",
        "                    distances = [cdist(np.array([current_embedding]), np.array([sp['voiceprint']]), 'cosine')[0][0] for sp in enrolled_speakers]\n",
        "                    min_distance = np.min(distances)\n",
        "\n",
        "                    if min_distance < DISTANCE_THRESHOLD:\n",
        "                        best_match_index = np.argmin(distances)\n",
        "                        speaker_name = enrolled_speakers[best_match_index]['name']\n",
        "                        speaker_mapping[speaker_label] = speaker_name\n",
        "                    else:\n",
        "                        if speaker_label not in unidentified_speakers:\n",
        "                            unidentified_speakers[speaker_label] = f\"Speaker {generic_speaker_count}\"\n",
        "                            generic_speaker_count += 1\n",
        "                        speaker_mapping[speaker_label] = unidentified_speakers[speaker_label]\n",
        "\n",
        "                    if os.path.exists(temp_segment_path):\n",
        "                        os.remove(temp_segment_path)\n",
        "        else:\n",
        "            st.info(\"No speakers enrolled. Using generic labels (Speaker 1, Speaker 2, etc.).\")\n",
        "            generic_labels = sorted(list(set([label for _, _, label in diarization.itertracks(yield_label=True)])))\n",
        "            for i, label in enumerate(generic_labels):\n",
        "                speaker_mapping[label] = f\"Speaker {i + 1}\"\n",
        "        # --- END OF NEW LOGIC ---\n",
        "\n",
        "        result = whisper_model.transcribe(diarize_temp_path, fp16=True, word_timestamps=True)\n",
        "        word_segments = result['segments']\n",
        "\n",
        "        transcript_parts = []\n",
        "        for turn, _, speaker_label in diarization.itertracks(yield_label=True):\n",
        "            turn_start, turn_end = turn.start, turn.end\n",
        "            speaker_name = speaker_mapping.get(speaker_label, \"Unknown\")\n",
        "\n",
        "            turn_words = []\n",
        "            for segment in word_segments:\n",
        "                for word in segment['words']:\n",
        "                    if turn_start <= word['start'] < turn_end:\n",
        "                        turn_words.append(word['word'])\n",
        "\n",
        "            if turn_words:\n",
        "                turn_text = \"\".join(turn_words).strip()\n",
        "                transcript_parts.append(f\"**{speaker_name}**: {turn_text}\")\n",
        "\n",
        "        final_transcript = \"\\n\\n\".join(transcript_parts)\n",
        "\n",
        "        prompt = GEMINI_JSON_PROMPT.replace(\"[INSERT TRANSCRIPT HERE]\", final_transcript)\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "\n",
        "        cleaned_response = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        summary_data = json.loads(cleaned_response)\n",
        "        summary_data['full_transcript'] = final_transcript\n",
        "        return summary_data\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during processing: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if os.path.exists(diarize_temp_path):\n",
        "            os.remove(diarize_temp_path)\n",
        "\n",
        "# --- STREAMLIT UI ---\n",
        "st.title(\"üß† AI Meeting Summarizer\")\n",
        "st.write(\"This tool transcribes, identifies speakers, and summarizes meetings. Built using Colab and Google Drive.\")\n",
        "\n",
        "if 'enrolled_speakers' not in st.session_state:\n",
        "    st.session_state.enrolled_speakers = load_enrolled_speakers()\n",
        "\n",
        "if not models_loaded:\n",
        "    st.stop()\n",
        "\n",
        "app_mode = st.sidebar.radio(\"Choose an action\", [\"Process Meeting\", \"Enroll New Speaker\"])\n",
        "\n",
        "if app_mode == \"Enroll New Speaker\":\n",
        "    st.header(\"Enroll a New Speaker\")\n",
        "    st.info(\"Upload a short, clear audio sample (15-30 seconds) of a single person speaking.\")\n",
        "\n",
        "    with st.form(\"enroll_form\"):\n",
        "        speaker_name = st.text_input(\"Enter Speaker's Name:\")\n",
        "        audio_sample = st.file_uploader(\"Upload Audio Sample\", type=[\"wav\", \"mp3\", \"m4a\"])\n",
        "        submitted = st.form_submit_button(\"Enroll Speaker\")\n",
        "\n",
        "    if submitted and speaker_name and audio_sample:\n",
        "        with st.spinner(f\"Enrolling {speaker_name}...\"):\n",
        "            message = enroll_speaker(speaker_name, audio_sample)\n",
        "            st.success(message)\n",
        "\n",
        "elif app_mode == \"Process Meeting\":\n",
        "    st.header(\"Process a Meeting\")\n",
        "    st.info(\"Upload the full meeting audio recording to get a summary and transcript.\")\n",
        "\n",
        "    meeting_audio = st.file_uploader(\"Upload Meeting Audio\", type=[\"wav\", \"mp3\", \"m4a\"])\n",
        "\n",
        "    if st.button(\"Analyze Meeting\") and meeting_audio:\n",
        "        st.write(\"---\")\n",
        "        with st.spinner(\"Analyzing meeting... This may take several minutes on CPU, much faster on GPU.\"):\n",
        "            results = process_meeting(meeting_audio)\n",
        "\n",
        "        if results:\n",
        "            st.header(\"Meeting Results\")\n",
        "            st.subheader(\"üìå Title\")\n",
        "            st.write(f\"**{results.get('title', 'N/A')}**\")\n",
        "\n",
        "            st.subheader(\"üìÑ Summary\")\n",
        "            st.write(f\"{results.get('summary', 'N/A')}\")\n",
        "\n",
        "            st.subheader(\"‚úÖ Action Items\")\n",
        "            for item in results.get('action_items', []):\n",
        "                st.markdown(f\"- **Task:** {item['task']} | **Assigned to:** {item['assigned_to']} | **Deadline:** {item['deadline']}\")\n",
        "\n",
        "            st.subheader(\"üîë Key Decisions\")\n",
        "            for decision in results.get('key_decisions', []):\n",
        "                st.markdown(f\"- {decision}\")\n",
        "\n",
        "            with st.expander(\"Show Full Transcript\"):\n",
        "                st.markdown(results.get('full_transcript', ''))\n",
        "\n",
        "# Display enrolled speakers in the sidebar\n",
        "st.sidebar.write(\"---\")\n",
        "st.sidebar.subheader(\"Enrolled Speakers\")\n",
        "if st.session_state.enrolled_speakers:\n",
        "    for speaker in st.session_state.enrolled_speakers:\n",
        "        st.sidebar.markdown(f\"- {speaker['name']}\")\n",
        "else:\n",
        "    st.sidebar.info(\"No speakers enrolled yet.\")"
      ],
      "metadata": {
        "id": "u2VsYCXKoTrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- Step 1: Kill any old ngrok tunnels to prevent errors ---\n",
        "ngrok.kill()\n",
        "print(\"‚úÖ Old ngrok tunnels killed.\")\n",
        "\n",
        "# --- Step 2: Load your secrets from Colab ---\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "print(\"‚úÖ Secrets loaded.\")\n",
        "\n",
        "# --- Step 3: Launch the Streamlit app in the background ---\n",
        "# We pass the secrets as environment variables to the command\n",
        "!HF_TOKEN={HF_TOKEN} GEMINI_API_KEY={GEMINI_API_KEY} streamlit run app.py &>streamlit.log &\n",
        "print(\"üöÄ Streamlit app launching in the background...\")\n",
        "\n",
        "# --- Step 4: Wait for 5 seconds for the app to start ---\n",
        "time.sleep(5)\n",
        "print(\"‚è≥ Waiting for app to initialize...\")\n",
        "\n",
        "# --- Step 5: Connect ngrok to the now-running app ---\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"\\nüéâ Your Streamlit app is now live at: {public_url}\")"
      ],
      "metadata": {
        "id": "cVcmCTtuoU13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}